{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "USI ML 19 RL A4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-3gmuwlmdyO4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1kKJk_rJC34",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning 2019/2020: Assignment 4 -  Reinforcement Learning\n",
        "Deadline: Friday 6th of December 2019 9pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLtJ0vlKJReh",
        "colab_type": "text"
      },
      "source": [
        "First name: XXX  \n",
        "Last name: XXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8xajf5MJ8Ud",
        "colab_type": "text"
      },
      "source": [
        "## About this assignment\n",
        "\n",
        "In this assignment you will further deepen your understanding of Reinforcement Learning (RL).\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "Please write your answers, equations, and code directly in this python notebook and print the final result to pdf (File > Print).\n",
        "Make sure that code has appropriate line breaks such that all code is visible in the final pdf.\n",
        "Also select A3 for the PDF size to prevent content from being clipped.\n",
        "\n",
        "The final pdf must be named name.lastname.pdf and uploaded to the iCorsi website before the deadline expires. Late submissions will result in 0 points.\n",
        "\n",
        "**Also share this notebook (top right corner 'Share') with teaching.idsia@gmail.com during submission.**\n",
        "\n",
        "**Keep your answers brief and respect the sentence limits in each question (answers exceeding the limit are not taken into account)**.\n",
        "\n",
        "Learn more about python notebooks and formatting here: https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "## How to get help\n",
        "\n",
        "We encourage you to use the tutorials to ask questions or to discuss exercises with other students.\n",
        "However, do not look at any report written by others or share your report with others. Violation of that rule will result in 0 points for all students involved. For further questions you can send an email to louis@idsia.ch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3uo_XEzLkxc",
        "colab_type": "text"
      },
      "source": [
        "## 1 Basic probability (6p)\n",
        "\n",
        "Suppose that a migrating lizard that rests in Ticino can be in four different states:\n",
        "Eating (E), Sleeping (S), Fighting (F) and Mating (M), for example protecting its territory against other lizards. Each lizard spends 30% of its time sleeping, 40% eating, 20% fighting and the remaining time mating. A biologist collects a population of lizards and puts them in a cage to study their behaviors. Suppose the probability for a lizard being caught while eating is 0.1, for a sleeping lizard 0.4, for a fighting lizard 0.8 and for the lizards that are mating 0.2, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRg4g3_NMXVI",
        "colab_type": "text"
      },
      "source": [
        "### Question 1.1 (3p)\n",
        "What is the relative frequency (probability) for a lizard being caught in the cage?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OO7HpEwMdnH",
        "colab_type": "text"
      },
      "source": [
        "### Question 1.2 (3p)\n",
        "\n",
        "What is the proportion of lizards that are fighting of those that were caught in the cage?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okJpynzHMpA6",
        "colab_type": "text"
      },
      "source": [
        "## 2 Markov Decision Processes (32p)\n",
        "\n",
        "Suppose a robot is put in a maze with long corridor. The\n",
        "corridor is 1 kilometer long and 5 meters wide. The available actions to the robot are moving forward for 1 meter, moving backward for 1 meter, turning left for 90 degrees and turning right for 90 degrees. If the robot moves and hits the wall, then it will stay in its position and orientation. The robot's goal is to escape from this maze by reaching the end of the long corridor.\n",
        "**Note: the answers in the following questions should not exceed 5 sentences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI3AATMANzC_",
        "colab_type": "text"
      },
      "source": [
        "### Question 2.1 (4p)\n",
        "\n",
        "Assume the robot receives a +1 reward signal for each time step taken in the\n",
        "maze and +1000 for reaching the final goal (the end of the long corridor). Then you train the robot for a while, but it seems it still does not perform well at all for navigating to the end of the corridor in the maze. What is happening? Is there something wrong with the reward function?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC-aYmiFOAEZ",
        "colab_type": "text"
      },
      "source": [
        "### Question 2.2 (4p)\n",
        "\n",
        "If there is something wrong with the reward function, how could you fix it? If not, how to resolve the training issues?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzIDv8qoOGuH",
        "colab_type": "text"
      },
      "source": [
        "### Questions 2.3 (2p)\n",
        "\n",
        "The discounted return for a non-episodic task is defined as\n",
        "$$\n",
        "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n",
        "$$\n",
        "where $\\gamma \\in [0, 1]$ is the discount factor.\n",
        "\n",
        "Rewrite the above equation such that $G_t$ is on the left hand side and $G_{t+1}$ is on the right hand side.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HTxEimD5O5eA"
      },
      "source": [
        "### Questions 2.4 (2p)\n",
        "\n",
        "What is the sufficient condition for this infinite series to be a convergent series?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bp30j2csPJkz"
      },
      "source": [
        "### Questions 2.5 (5p)\n",
        "\n",
        "Suppose this infinite series is a convergent series, and each reward in the series is a constant of +1. We know the series is bounded, what is a simple formula for this bound ? Write it down without using summation.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bmDQKWHNPSnx"
      },
      "source": [
        "### Questions 2.6 (5p)\n",
        "\n",
        "Let the task be an episodic setting and the robot is running for $T = 5$ time steps. Suppose $\\gamma = 0.3$, and the robot receives rewards along the way $R_1 = −1, R_2 = −0.5, R_3 = 2, R_4 = 1, R_5 = 6$. What are the values for $G_0, G_1, G_2, G_3, G_4, G_5$?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G7ZcAEvfQA8f"
      },
      "source": [
        "### Questions 2.7 (5p)\n",
        "\n",
        "Suppose each reward in the series is increased by a constant $c$, i.e. $R_t \\leftarrow R_t + c$.\n",
        "Then how does it change $G_t$?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j7KPVdhPQBpz"
      },
      "source": [
        "### Questions 2.8 (5p)\n",
        "\n",
        "Now consider episodic tasks, and similar to Question 2.7, we add a constant $c$ to each reward, how does it change $G_t$?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjOnM7AVQVOB",
        "colab_type": "text"
      },
      "source": [
        "## 3 Dynamic Programming (62p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Jm0EyIHQaOp"
      },
      "source": [
        "### Questions 3.1 (5p)\n",
        "\n",
        "Write down the Bellman optimality equation for the state value function without using expectation notation, but using probability distributions instead. \n",
        "Define all variables and probability distributions in bullet points.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f0VZcK6LQkUC"
      },
      "source": [
        "### Questions 3.2 (5p)\n",
        "\n",
        "Write down the Bellman optimality equation for the state-action value function without using expectation notation, but using probability distributions instead.\n",
        "Define all variables and probability distributions in bullet points.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tq66sRJeQlE2"
      },
      "source": [
        "### Questions 3.3 (15p)\n",
        "\n",
        "Consider a 4x4 gridworld depicted in the following table:\n",
        "\n",
        "![Grid world](https://i.ibb.co/HdSdKJB/image.png)\n",
        "\n",
        "The non-terminal states are $S = \\{1, 2, \\ldots, 14\\}$ and the terminal states are $\\bar S = \\{0, 15\\}$.\n",
        "There are four available actions for each state, that is $A = \\{\\text{up}, \\text{down}, \\text{left}, \\text{right}\\}$.\n",
        "Assume the state transitions are deterministic and all transitions result in a negative reward −1 (after termination all rewards are zero).\n",
        "If the agent hits the boundary, then its state will remain unchanged, e.g. $p(s=8, r=−1|s=8, a=\\text{left}) = 1$.\n",
        "Note: In this exercise, we assume the policy is a deterministic\n",
        "function.\n",
        "\n",
        "Manually run the policy iteration algorithm (see lecture slide 58) for one iteration. Use the in-place policy iteration algorithm.\n",
        "This means one time policy evaluation with a single pass through the states (16 equations) and one time policy improvement.\n",
        "Assume the initial state value for all 16 cells are 0.0 and the policy initially always outputs the 'left' action.\n",
        "Write down the equations and detailed numerical computations for the updated values of each cell.\n",
        "Use a discount factor $\\gamma = 0.5$.\n",
        "Write down the policy after policy improvement.\n",
        "\n",
        "![Policy iteration](http://www.incompleteideas.net/book/ebook/imgtmp5.png)\n",
        "\n",
        "Read more about this in Sutton & Barto's book http://www.incompleteideas.net/book/ebook/node43.html\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ciourK1wQo-3"
      },
      "source": [
        "### Questions 3.4 (15p)\n",
        "\n",
        "Implement the beforementioned environment in the code skeleton below.\n",
        "Come up with your own solution and do not copy the code from a third party source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3gmuwlmdyO4",
        "colab_type": "text"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkzicBh-I3dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "np.set_printoptions(precision=3, linewidth=180)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_bNpgqd2mM",
        "colab_type": "text"
      },
      "source": [
        "#### Defining the problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNBoBp3PJC0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GridWorld:\n",
        "  UP = 0\n",
        "  DOWN = 1\n",
        "  LEFT = 2\n",
        "  RIGHT = 3\n",
        "\n",
        "  def __init__(self, side=4):\n",
        "    self.side = side\n",
        "    # -------------------------\n",
        "    # Define integer states, actions, and final states as specified in the problem description\n",
        "\n",
        "    # TODO insert code here\n",
        "    self.actions = ...\n",
        "    self.states = ...\n",
        "    self.finals = ...\n",
        "\n",
        "    # -------------------------\n",
        "    self.actions_repr = np.array(['↑', '↓', '←', '→'])\n",
        "\n",
        "  def reward(self, s, s_next, a):\n",
        "    # -------------------------\n",
        "    # Return the reward for the given transition as specified in the problem description\n",
        "\n",
        "    # TODO insert code here\n",
        "\n",
        "    # -------------------------\n",
        "\n",
        "  def transition_prob(self, s, s_next, a):\n",
        "    # -------------------------\n",
        "    # Return a probability in [0, 1] for the given transition as specified in the problem description\n",
        "\n",
        "    # TODO insert code here\n",
        "\n",
        "    # -------------------------\n",
        "\n",
        "  def print_policy(self, policy):\n",
        "    P = np.array(policy).reshape(self.side, self.side)\n",
        "    print(self.actions_repr[P])\n",
        "  \n",
        "  def print_values(self, values):\n",
        "    V = np.array(values).reshape(self.side, self.side)\n",
        "    print(V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FqMt-0yAuGz8"
      },
      "source": [
        "### Questions 3.5 (17p)\n",
        "\n",
        "Implement policy iteration in the code skeleton below.\n",
        "Come up with your own solution and do not copy the code from a third party source.\n",
        "\n",
        "Run the code multiple times. Do you always end up with the same policy? Why? (max 4 sentences)\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrUMxh-qd5u0",
        "colab_type": "text"
      },
      "source": [
        "#### Policy iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1DOXcH5J0NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_policy(world, policy, values, gamma=0.9, theta=0.01):\n",
        "  # --------------------------\n",
        "  # Implement policy evaluation and return the updated value function\n",
        "\n",
        "  # TODO insert code here\n",
        "\n",
        "  # --------------------------\n",
        "\n",
        "\n",
        "def improve_policy(world, policy, values, gamma=0.9):\n",
        "  # --------------------------\n",
        "  # Implement policy improvement and return the updated policy\n",
        "\n",
        "  # TODO insert code here\n",
        "\n",
        "  # --------------------------\n",
        "\n",
        "\n",
        "def policy_iteration(world, gamma=0.9, theta=0.01):\n",
        "  # Initialize a random policy\n",
        "  policy = np.array([np.random.choice(world.actions) for s in world.states])\n",
        "  print('Initial policy')\n",
        "  world.print_policy(policy)\n",
        "  # Initialize values to zero\n",
        "  values = np.zeros_like(world.states, dtype=np.float32)\n",
        "\n",
        "  # Run policy iteration\n",
        "  stable = False\n",
        "  for i in itertools.count():\n",
        "    print(f'Iteration {i}')\n",
        "    values = eval_policy(world, policy, values, gamma, theta)\n",
        "    world.print_values(values)\n",
        "    stable = improve_policy(world, policy, values, gamma)\n",
        "    world.print_policy(policy)\n",
        "    if stable:\n",
        "      break\n",
        "\n",
        "  return policy, values\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqdoNw97mcEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate your code, please include the output in your submission\n",
        "world = GridWorld()\n",
        "policy, values = policy_iteration(world, gamma=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1hRFm6Zm6YI1"
      },
      "source": [
        "### Questions 3.5 (5p)\n",
        "\n",
        "Let's run policy iteration with $\\gamma = 1$. Describe what is happening. Why is this the case? Give an example. What is $\\gamma$ trading off and how does it affect policy iteration? (max 8 sentences)\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1rAQ1K_u6qtH",
        "colab": {}
      },
      "source": [
        "policy_iteration(problem, gamma=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}